[version introduce]
For experiment2 : 
The performance of different combination of attention vector and lstm output

experiment1:
1.train one additional weight for sim, attention weight is generated by one minus sim weight

experiment3:
1.pretrained with lstm 
2.hidden softmax pretrained with lstm softmax
3.read softmax pretrained with lstm softmax

experiment4:
1.all train together

[experiment2 version]
1.generated attention vector are attached to a linear layer to project to the lstm output space. The projection vector and the output of lstm are directly summed up and attached to softmax.
